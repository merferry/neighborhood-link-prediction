Link prediction aims to anticipate the existence of missing/future connection between nodes based on known interactions and network structure \cite{arrar2023comprehensive}. Similarity measures are commonly used for their simplicity, interpretability \cite{pai2019netdx, barbieri2014follow}, and computational efficiency \cite{garcia2014link}. Further, they are often combined with other approaches \cite{kumari2022supervised, abuoda2020link, pai2019netdx}, such as ensemble learning \cite{zhou2012ensemble}.

However, evaluating these algorithms on large networks is crucial for accurate insights \cite{zhou2021progresses, zhou2021experimental}. Many works in this field focus on small \cite{guo2019node, rafiee2020cndp, mumin2022efficient, papadimitriou2012fast, vega2021link, saifi2023fast, ferreira2019scalability, benhidour2022approach} to medium-scale graphs \cite{yang2015new, cui2016bounded, kalkanfinding, mohan2017scalable, wang2019link, bastami2019gravitation, shin2012multi, garcia2014link}, while parallelism becomes essential for larger networks. This technical report addresses both issues and introduces a heuristic for efficient computation. Further, a number of research studies \cite{gatadi2023lpcd, saifi2023fast, benhidour2022approach, mumin2022efficient, rafiee2020cndp, guo2019node, yang2015new, papadimitriou2012fast, wang2019link} and popular network analysis software (such as NetworKit \cite{staudt2016networkit} and igraph \cite{csardi2006igraph}), use the baseline approach, which computes similarity scores for all non-connected node pairs and predicts links based on the top-$k$ scores. Despite its simplicity, this approach incurs unnecessary computational costs as many node pairs lack common neighbors, and has a high time complexity\ignore{of $O(N^2D)$}.

In this report, we introduced the IHub approach, an enhanced parallel method that efficiently finds common neighbors and handles large graphs by tracking top-$k$ edges per-thread and later merging them globally.\ignore{This allows it to run on large graphs.} Additionally, we presented the LHub heuristic approach, which disregards large hubs. It is based on the notion that low-degree nodes contribute significant similarity among neighbors, in contrast to high-degree nodes (interestingly, this is similar to the idea behind the AA and RA similarity metrics). We then experimentally determined suitable hub limits for link prediction with each similarity metric.

Our results indicate that the LHub approach is, on average, over $1622\times$ and $415\times$ faster than the IHub approach with $10^{-2}|E|$ and $0.1|E|$ unobserved edges, respectively. It achieves this speedup while predicting links with an average F1 score that is $80\%$ higher and $13\%$ lower, respectively --- meaning similar F1 scores without being too low or high. Notably, on the \textit{sk-2005} graph with $0.1|E|$ edges removed, LHub achieves a link prediction rate of $38.1M$ edges/s.

Moreover, RA metric shows superior performance in terms of F1 score and runtime on web graphs and social networks using the IHub approach. On road networks and protein k-mer graphs, JC similarity metric outperforms others with the IHub approach. With the LHub approach and $10^{-2}|E|$ unobserved edges, CN metric (hub limit $L_H$ of $32$) excels on web graphs and social networks, LHN metric (hub limit $L_H$ of $4$) performs best on road networks, and JC metric (hub limit $L_H$ of $256$) is optimal for protein k-mer graphs. However, with $0.1|E|$ unobserved edges, CN metric (hub limit $L_H$ of $32$) is the best across all graphs. LHub outperforms IHub, achieving higher F1 scores, particularly on web graphs and social networks.

When predicting $0.1|E|$ edges with the LHub approach, we observed that $63\%$ of the runtime is spent on the scoring phase, and especially higher on social networks, with high average degree. However, a significant portion is also dedicated to the merging phase, suggesting potential optimization opportunities for future work. In the strong scaling analysis, the LHub approach achieves a speedup of $10.4\times$ with 32 threads, indicating a performance increase of $1.6\times$ for every doubling of threads. However, scalability is constrained, particularly due to the merging phase\ignore{which achieves no speedup due to increasing work, and because it is sequential}.

In the future, we want to explore optimizing quasi-local and global methods of similarity. While we explore second order neighbors in this report, our techniques can be extended to apply to higher order neighbors.

\ignore{Link prediction is essentially a compression problem, i.e. understanding the given network, and then predicting the missing links. As long as the network is not fully deterministic, one should not expect link prediction algorithms to be $100\%$ precise. Real-world networks lie somewhere between being completely deterministic, and completely random. It is possible that applying link prediction to multiple graphs of similar nature can help improve the accuracy of a link prediction algorithm. Note that explainabilty of link prediction algorithms is important.}
